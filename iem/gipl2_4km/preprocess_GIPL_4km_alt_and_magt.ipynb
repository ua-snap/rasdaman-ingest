{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbae3a09",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "The purpose of this notebook is to retreive and pre-process the GIPL 2.0 model outputs described by the following publication:\n",
    "\n",
    "<i>\n",
    "Climate damages to Alaska public infrastructure\n",
    "April M. Melvin, Peter Larsen, Brent Boehlert, James E. Neumann, Paul Chinowsky, Xavier Espinet, Jeremy Martinich, Matthew S. Baumann, Lisa Rennels, Alexandra Bothner, Dmitry J. Nicolsky, Sergey S. Marchenko\n",
    "Proceedings of the National Academy of Sciences Jan 2017, 114 (2) E122-E131; DOI: 10.1073/pnas.1611056113\n",
    "</i>\n",
    "\n",
    "The goal is ingest the data as a netCDF to Rasdaman and then create an endpoint around the data in the SNAP Data API for use in the [Northern Climate Reports](https://northerclimatereports.org) app.\n",
    "\n",
    "\n",
    "## Background and Motivation\n",
    "Data represent RCP 4.5 and 8.5 scenarios across 5 climate models. Outputs are binned into \"eras\" and there are two GIPL permafrost variables Active Layer Thickness (ALT) and Mean Annual Ground Temperature (MAGT).\n",
    "The goal is to create a completely inclusive datacube of both historical and projected data. \n",
    "It will have the following dimensions for both ALT and MAGT variables:\n",
    "* era\n",
    "* model\n",
    "* scenario\n",
    "* Y\n",
    "* X\n",
    "\n",
    "# Steps\n",
    " - Download data from the [Google Drive folder shared by Jeremy Littell](https://drive.google.com/file/d/1NaZInm20tpnv63NcC5_p3xTWp5gC3Ty6/view?usp=sharing]) to the `input_data` directory.\n",
    " - Convert the ASCII raster data to GeoTIFF using the accompanying projection information contained in the conjugate .prj file. There is one .prj file for each ASCII file, but they are all the same.\n",
    " - Encode era, climate model, and emission scenario information in the GeoTIFF file name.\n",
    " - Crop the GeoTiffs to a [known Alaskan spatial boundary](https://github.com/ua-snap/geospatial-vector-veracity/tree/main/vector_data/polygon/boundaries/alaska_coast_simplified)\n",
    " - Convert the GeoTIFF stack to a multidimensional netCDF for ingest to Rasdaman.\n",
    " \n",
    "\n",
    "## References\n",
    " - [Melvin et al., 2017Z](https://doi.org/10.1073/pnas.1611056113)\n",
    "   - [Supporting Information](https://www.pnas.org/content/pnas/suppl/2016/12/21/1611056113.DCSupplemental/pnas.201611056SI.pdf?targetid=nameddest%3DSTXT)\n",
    " - [README: ASCII file naming convention key (J. Littel)](https://docs.google.com/document/d/1OdoOqu8pFLnjbyWC_0f5ev8fS-ilZ3pD/edit#heading=h.gjdgxs) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a002c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALT_10_1.asc  ALT_16_3.prj  ALT_6_2.asc    MAGT_13_2.prj  MAGT_2_3.asc\r\n",
      "ALT_10_1.prj  ALT_16_4.asc  ALT_6_2.prj    MAGT_13_3.asc  MAGT_2_3.prj\r\n",
      "ALT_10_2.asc  ALT_16_4.prj  ALT_6_3.asc    MAGT_13_3.prj  MAGT_2_4.asc\r\n",
      "ALT_10_2.prj  ALT_17_1.asc  ALT_6_3.prj    MAGT_13_4.asc  MAGT_2_4.prj\r\n",
      "ALT_10_3.asc  ALT_17_1.prj  ALT_6_4.asc    MAGT_13_4.prj  MAGT_3_1.asc\r\n",
      "ALT_10_3.prj  ALT_17_2.asc  ALT_6_4.prj    MAGT_1_3.asc   MAGT_3_1.prj\r\n",
      "ALT_10_4.asc  ALT_17_2.prj  ALT_7_1.asc    MAGT_1_3.prj   MAGT_3_2.asc\r\n",
      "ALT_10_4.prj  ALT_17_3.asc  ALT_7_1.prj    MAGT_14_1.asc  MAGT_3_2.prj\r\n",
      "ALT_11_1.asc  ALT_17_3.prj  ALT_7_2.asc    MAGT_14_1.prj  MAGT_3_3.asc\r\n",
      "ALT_11_1.prj  ALT_17_4.asc  ALT_7_2.prj    MAGT_14_2.asc  MAGT_3_3.prj\r\n",
      "ALT_11_2.asc  ALT_17_4.prj  ALT_7_3.asc    MAGT_14_2.prj  MAGT_3_4.asc\r\n",
      "ALT_11_2.prj  ALT_18_1.asc  ALT_7_3.prj    MAGT_14_3.asc  MAGT_3_4.prj\r\n",
      "ALT_11_3.asc  ALT_18_1.prj  ALT_7_4.asc    MAGT_14_3.prj  MAGT_4_1.asc\r\n",
      "ALT_11_3.prj  ALT_18_2.asc  ALT_7_4.prj    MAGT_14_4.asc  MAGT_4_1.prj\r\n",
      "ALT_11_4.asc  ALT_18_2.prj  ALT_8_1.asc    MAGT_14_4.prj  MAGT_4_2.asc\r\n",
      "ALT_11_4.prj  ALT_18_3.asc  ALT_8_1.prj    MAGT_1_4.asc   MAGT_4_2.prj\r\n",
      "ALT_1_1.asc   ALT_18_3.prj  ALT_8_2.asc    MAGT_1_4.prj   MAGT_4_3.asc\r\n",
      "ALT_1_1.prj   ALT_18_4.asc  ALT_8_2.prj    MAGT_15_1.asc  MAGT_4_3.prj\r\n",
      "ALT_12_1.asc  ALT_18_4.prj  ALT_8_3.asc    MAGT_15_1.prj  MAGT_4_4.asc\r\n",
      "ALT_12_1.prj  ALT_19_1.asc  ALT_8_3.prj    MAGT_15_2.asc  MAGT_4_4.prj\r\n",
      "ALT_12_2.asc  ALT_19_1.prj  ALT_8_4.asc    MAGT_15_2.prj  MAGT_5_1.asc\r\n",
      "ALT_12_2.prj  ALT_19_2.asc  ALT_8_4.prj    MAGT_15_3.asc  MAGT_5_1.prj\r\n",
      "ALT_12_3.asc  ALT_19_2.prj  ALT_9_1.asc    MAGT_15_3.prj  MAGT_5_2.asc\r\n",
      "ALT_12_3.prj  ALT_19_3.asc  ALT_9_1.prj    MAGT_15_4.asc  MAGT_5_2.prj\r\n",
      "ALT_12_4.asc  ALT_19_3.prj  ALT_9_2.asc    MAGT_15_4.prj  MAGT_5_3.asc\r\n",
      "ALT_12_4.prj  ALT_19_4.asc  ALT_9_2.prj    MAGT_16_1.asc  MAGT_5_3.prj\r\n",
      "ALT_1_2.asc   ALT_19_4.prj  ALT_9_3.asc    MAGT_16_1.prj  MAGT_5_4.asc\r\n",
      "ALT_1_2.prj   ALT_2_1.asc   ALT_9_3.prj    MAGT_16_2.asc  MAGT_5_4.prj\r\n",
      "ALT_13_1.asc  ALT_2_1.prj   ALT_9_4.asc    MAGT_16_2.prj  MAGT_6_1.asc\r\n",
      "ALT_13_1.prj  ALT_2_2.asc   ALT_9_4.prj    MAGT_16_3.asc  MAGT_6_1.prj\r\n",
      "ALT_13_2.asc  ALT_2_2.prj   MAGT_10_1.asc  MAGT_16_3.prj  MAGT_6_2.asc\r\n",
      "ALT_13_2.prj  ALT_2_3.asc   MAGT_10_1.prj  MAGT_16_4.asc  MAGT_6_2.prj\r\n",
      "ALT_13_3.asc  ALT_2_3.prj   MAGT_10_2.asc  MAGT_16_4.prj  MAGT_6_3.asc\r\n",
      "ALT_13_3.prj  ALT_2_4.asc   MAGT_10_2.prj  MAGT_17_1.asc  MAGT_6_3.prj\r\n",
      "ALT_13_4.asc  ALT_2_4.prj   MAGT_10_3.asc  MAGT_17_1.prj  MAGT_6_4.asc\r\n",
      "ALT_13_4.prj  ALT_3_1.asc   MAGT_10_3.prj  MAGT_17_2.asc  MAGT_6_4.prj\r\n",
      "ALT_1_3.asc   ALT_3_1.prj   MAGT_10_4.asc  MAGT_17_2.prj  MAGT_7_1.asc\r\n",
      "ALT_1_3.prj   ALT_3_2.asc   MAGT_10_4.prj  MAGT_17_3.asc  MAGT_7_1.prj\r\n",
      "ALT_14_1.asc  ALT_3_2.prj   MAGT_11_1.asc  MAGT_17_3.prj  MAGT_7_2.asc\r\n",
      "ALT_14_1.prj  ALT_3_3.asc   MAGT_11_1.prj  MAGT_17_4.asc  MAGT_7_2.prj\r\n",
      "ALT_14_2.asc  ALT_3_3.prj   MAGT_11_2.asc  MAGT_17_4.prj  MAGT_7_3.asc\r\n",
      "ALT_14_2.prj  ALT_3_4.asc   MAGT_11_2.prj  MAGT_18_1.asc  MAGT_7_3.prj\r\n",
      "ALT_14_3.asc  ALT_3_4.prj   MAGT_11_3.asc  MAGT_18_1.prj  MAGT_7_4.asc\r\n",
      "ALT_14_3.prj  ALT_4_1.asc   MAGT_11_3.prj  MAGT_18_2.asc  MAGT_7_4.prj\r\n",
      "ALT_14_4.asc  ALT_4_1.prj   MAGT_11_4.asc  MAGT_18_2.prj  MAGT_8_1.asc\r\n",
      "ALT_14_4.prj  ALT_4_2.asc   MAGT_11_4.prj  MAGT_18_3.asc  MAGT_8_1.prj\r\n",
      "ALT_1_4.asc   ALT_4_2.prj   MAGT_1_1.asc   MAGT_18_3.prj  MAGT_8_2.asc\r\n",
      "ALT_1_4.prj   ALT_4_3.asc   MAGT_1_1.prj   MAGT_18_4.asc  MAGT_8_2.prj\r\n",
      "ALT_15_1.asc  ALT_4_3.prj   MAGT_12_1.asc  MAGT_18_4.prj  MAGT_8_3.asc\r\n",
      "ALT_15_1.prj  ALT_4_4.asc   MAGT_12_1.prj  MAGT_19_1.asc  MAGT_8_3.prj\r\n",
      "ALT_15_2.asc  ALT_4_4.prj   MAGT_12_2.asc  MAGT_19_1.prj  MAGT_8_4.asc\r\n",
      "ALT_15_2.prj  ALT_5_1.asc   MAGT_12_2.prj  MAGT_19_2.asc  MAGT_8_4.prj\r\n",
      "ALT_15_3.asc  ALT_5_1.prj   MAGT_12_3.asc  MAGT_19_2.prj  MAGT_9_1.asc\r\n",
      "ALT_15_3.prj  ALT_5_2.asc   MAGT_12_3.prj  MAGT_19_3.asc  MAGT_9_1.prj\r\n",
      "ALT_15_4.asc  ALT_5_2.prj   MAGT_12_4.asc  MAGT_19_3.prj  MAGT_9_2.asc\r\n",
      "ALT_15_4.prj  ALT_5_3.asc   MAGT_12_4.prj  MAGT_19_4.asc  MAGT_9_2.prj\r\n",
      "ALT_16_1.asc  ALT_5_3.prj   MAGT_1_2.asc   MAGT_19_4.prj  MAGT_9_3.asc\r\n",
      "ALT_16_1.prj  ALT_5_4.asc   MAGT_1_2.prj   MAGT_2_1.asc   MAGT_9_3.prj\r\n",
      "ALT_16_2.asc  ALT_5_4.prj   MAGT_13_1.asc  MAGT_2_1.prj   MAGT_9_4.asc\r\n",
      "ALT_16_2.prj  ALT_6_1.asc   MAGT_13_1.prj  MAGT_2_2.asc   MAGT_9_4.prj\r\n",
      "ALT_16_3.asc  ALT_6_1.prj   MAGT_13_2.asc  MAGT_2_2.prj\r\n"
     ]
    }
   ],
   "source": [
    "ls input_data/Permafrost/Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27f5e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be69c8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJCS[\"Alaska_Albers_Equal_Area_Conic\",GEOGCS[\"GCS_North_American_1983\",DATUM[\"D_North_American_1983\",SPHEROID[\"GRS_1980\",6378137,298.257222101]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.017453292519943295]],PROJECTION[\"Albers\"],PARAMETER[\"False_Easting\",0],PARAMETER[\"False_Northing\",0],PARAMETER[\"central_meridian\",-154],PARAMETER[\"Standard_Parallel_1\",55],PARAMETER[\"Standard_Parallel_2\",65],PARAMETER[\"latitude_of_origin\",50],UNIT[\"Meter\",1]]"
     ]
    }
   ],
   "source": [
    "# Verify the CRS of the ASCII data and the general shape of each file\n",
    "ascii_proj = %cat input_data/Permafrost/Projections/MAGT_10_1.prj\n",
    "header = !head -n 6 input_data/Permafrost/Projections/MAGT_10_1.asc\n",
    "src = rio.open('input_data/Permafrost/Projections/MAGT_10_1.asc')\n",
    "meta = src.meta\n",
    "profile = src.profile\n",
    "shape = src.read(1).shape\n",
    "ascii_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfd88b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ncols     1609',\n",
       " 'nrows     593',\n",
       " 'xllcenter -2172223.20581',\n",
       " 'yllcenter 177412.93264',\n",
       " 'cellsize  4000',\n",
       " 'NODATA_value -9999']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6a9062d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(593, 1609)\n",
      "{'driver': 'AAIGrid', 'dtype': 'float32', 'nodata': -9999.0, 'width': 1609, 'height': 593, 'count': 1, 'crs': CRS.from_wkt('PROJCS[\"Alaska_Albers_Equal_Area_Conic\",GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",50],PARAMETER[\"longitude_of_center\",-154],PARAMETER[\"standard_parallel_1\",55],PARAMETER[\"standard_parallel_2\",65],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'), 'transform': Affine(4000.0, 0.0, -2174223.20581,\n",
      "       0.0, -4000.0, 2547412.9326400002), 'tiled': False}\n"
     ]
    }
   ],
   "source": [
    "print(shape)\n",
    "print(profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e816d4",
   "metadata": {},
   "source": [
    "Wow! Rasterio knows where to look when the file names of the .prj and .asc files have indentical prefixes. These data appear to be in uniform shape and rasterio \"knows\" how to handle them. No red flags here. Per the README doc referenced earlier, we'll need a look-up table to retain era / model / scenario info and to generate output filenames.\n",
    "\n",
    "```\n",
    "These asci files are labeled by Variable_<Model/Policy Combination>_<Era>\n",
    "\n",
    "Model-Scenario Combinations (10-19)\n",
    "10: MRI-CGCM3, rcp4.5\n",
    "11: MRI-CGCM3, rcp8.5\n",
    "12: IPSL-CM5A-LR, rcp4.5\n",
    "13: IPSL-CM5A-LR, rcp8.5\n",
    "14: GISS-E2-R, rcp 4.5\n",
    "15: GISS-E2-R, rcp 8.5\n",
    "16: GFDL-CM3, rcp4.5\n",
    "17: GFDL-CM3, rcp8.5\n",
    "18: CCSM4, rcp4.5\n",
    "19: CCSM4, rcp8.5\n",
    "Eras (1-4): 30 year eras surrounding the following center dates, except last era is truncated at 2100.\n",
    "1: 2025 (2011 - 2040)\n",
    "2: 2050 (2036 - 2065)\n",
    "3: 2075 (2061 – 2090)\n",
    "4: 2095 (2086 – 2100)\n",
    "\n",
    "The historical baseline is 1986-2005.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcdaa5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scenario_di = {10: \"MRICGCM3_rcp45\",\n",
    "                     11: \"MRICGCM3_rcp85\",\n",
    "                     12: \"IPSLCM5ALR_rcp45\",\n",
    "                     13: \"IPSLCM5ALR_rcp85\",\n",
    "                     14: \"GISSE2R_rcp45\",\n",
    "                     15: \"GISSE2R_rcp85\",\n",
    "                     16: \"GFDLCM3_rcp45\",\n",
    "                     17: \"GFDLCM3_rcp85\",\n",
    "                     18: \"NCARCCSM4_rcp45\",\n",
    "                     19: \"NCARCCSM4_rcp85\",\n",
    "                    }\n",
    "era_di = {1: \"era2025_2011to2040\",\n",
    "          2: \"era2050_2036to2065\",\n",
    "          3: \"era2075_2061to2090\",\n",
    "          4: \"era2095_2086to2100\",\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cf7c828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "# Grab all the file paths for PROJECTED data (historical data (n=2) are in a separate directory)\n",
    "target_dir = Path(\"input_data/Permafrost/Projections/\")\n",
    "out_dir = Path(\"input_data/Permafrost/geotiff/\")\n",
    "asc_fps = [fp.name for fp in target_dir.glob(\"*_*.asc\")]\n",
    "cmip5_asc_fps = [target_dir.joinpath(x) for x in asc_fps if int(x.split(\"_\")[1]) >= 10]\n",
    "n_cmip5_ascs = len(cmip5_asc_fps)\n",
    "print(n_cmip5_ascs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c152ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_new_filenames(output_dst, fp_list):\n",
    "    \"\"\"\n",
    "    Blapping together new filenames of the format variable_model_scenario_era.tif\n",
    "    \"\"\"\n",
    "    new_fps = []\n",
    "    for fp in fp_list:\n",
    "        pf_var, model_scenario_key, era_key = fp.name[:-4].split(\"_\")\n",
    "        new_fp = pf_var.lower() + \"_\" + model_scenario_di[int(model_scenario_key)].lower() + \"_\" + era_di[int(era_key)] + \".tif\"\n",
    "        new_fp = output_dst.joinpath(new_fp)\n",
    "        new_fps.append(new_fp)\n",
    "    return new_fps\n",
    "\n",
    "\n",
    "def read_raster(raster_fp):\n",
    "    \"\"\"\n",
    "    Read raster to numpy array.\n",
    "    Read a single raster with rasterio and store raster in memory as a numpy\n",
    "    array. Also reads and returns the profile which includes metadata and the neccessary information\n",
    "    to write congruent datasets.\n",
    "    Args:\n",
    "        raster_fp (str): filepath to raster\n",
    "    Returns:\n",
    "        arr (ndarray): array of raster values\n",
    "        profile (dict): metadata profile\n",
    "    \"\"\"\n",
    "\n",
    "    src = rio.open(raster_fp)\n",
    "    arr = src.read(1)\n",
    "    profile = src.profile\n",
    "    return (arr, profile)\n",
    "\n",
    "\n",
    "def write_raster(arr, outpath, profile):\n",
    "    \"\"\"\n",
    "    Write numpy array to disk as a raster with correct metadata.\n",
    "    Args:\n",
    "        arr (ndarray): array of raster values\n",
    "        outpath (str): output filename and path for raster\n",
    "        profile (dict): metadata for output\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "\n",
    "    with rio.open(outpath, 'w', **profile) as dst:\n",
    "        dst.write(arr, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6356df83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/Permafrost/geotiff/alt_mricgcm3_rcp45_era2025_2011to2040.tif\n",
      "input_data/Permafrost/geotiff/magt_mricgcm3_rcp85_era2025_2011to2040.tif\n",
      "input_data/Permafrost/geotiff/magt_ncarccsm4_rcp85_era2095_2086to2100.tif\n"
     ]
    }
   ],
   "source": [
    "geotiff_fps = make_new_filenames(out_dir, cmip5_asc_fps)\n",
    "print(geotiff_fps[0])\n",
    "print(geotiff_fps[44])\n",
    "print(geotiff_fps[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29bc6647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the ASCI data to geotiff\n",
    "#!mkdir input_data/Permafrost/geotiff\n",
    "for asc, gtiff in zip(cmip5_asc_fps, geotiff_fps):\n",
    "    arr, profile = read_raster(asc)\n",
    "    profile.update(driver=\"GTiff\", compress=\"lzw\")\n",
    "    write_raster(arr, gtiff, profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb3d24cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[38;5;13malt_gfdlcm3_rcp45_era2025_2011to2040.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_gfdlcm3_rcp45_era2050_2036to2065.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_gfdlcm3_rcp45_era2075_2061to2090.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_gfdlcm3_rcp45_era2095_2086to2100.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_gfdlcm3_rcp85_era2025_2011to2040.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_gfdlcm3_rcp85_era2050_2036to2065.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_gfdlcm3_rcp85_era2075_2061to2090.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_gfdlcm3_rcp85_era2095_2086to2100.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_gisse2r_rcp45_era2025_2011to2040.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_gisse2r_rcp45_era2050_2036to2065.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_gisse2r_rcp45_era2075_2061to2090.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_gisse2r_rcp45_era2095_2086to2100.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_gisse2r_rcp85_era2025_2011to2040.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_gisse2r_rcp85_era2050_2036to2065.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_gisse2r_rcp85_era2075_2061to2090.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_gisse2r_rcp85_era2095_2086to2100.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_ipslcm5alr_rcp45_era2025_2011to2040.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_ipslcm5alr_rcp45_era2050_2036to2065.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_ipslcm5alr_rcp45_era2075_2061to2090.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_ipslcm5alr_rcp45_era2095_2086to2100.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_ipslcm5alr_rcp85_era2025_2011to2040.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_ipslcm5alr_rcp85_era2050_2036to2065.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_ipslcm5alr_rcp85_era2075_2061to2090.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_ipslcm5alr_rcp85_era2095_2086to2100.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_mricgcm3_rcp45_era2025_2011to2040.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_mricgcm3_rcp45_era2050_2036to2065.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_mricgcm3_rcp45_era2075_2061to2090.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_mricgcm3_rcp45_era2095_2086to2100.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_mricgcm3_rcp85_era2025_2011to2040.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_mricgcm3_rcp85_era2050_2036to2065.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_mricgcm3_rcp85_era2075_2061to2090.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_mricgcm3_rcp85_era2095_2086to2100.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_ncarccsm4_rcp45_era2025_2011to2040.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_ncarccsm4_rcp45_era2050_2036to2065.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_ncarccsm4_rcp45_era2075_2061to2090.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_ncarccsm4_rcp45_era2095_2086to2100.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_ncarccsm4_rcp85_era2025_2011to2040.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_ncarccsm4_rcp85_era2050_2036to2065.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_ncarccsm4_rcp85_era2075_2061to2090.tif\u001b[0m\r\n",
      "\u001b[38;5;13malt_ncarccsm4_rcp85_era2095_2086to2100.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_gfdlcm3_rcp45_era2025_2011to2040.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_gfdlcm3_rcp45_era2050_2036to2065.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_gfdlcm3_rcp45_era2075_2061to2090.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_gfdlcm3_rcp45_era2095_2086to2100.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_gfdlcm3_rcp85_era2025_2011to2040.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_gfdlcm3_rcp85_era2050_2036to2065.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_gfdlcm3_rcp85_era2075_2061to2090.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_gfdlcm3_rcp85_era2095_2086to2100.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_gisse2r_rcp45_era2025_2011to2040.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_gisse2r_rcp45_era2050_2036to2065.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_gisse2r_rcp45_era2075_2061to2090.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_gisse2r_rcp45_era2095_2086to2100.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_gisse2r_rcp85_era2025_2011to2040.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_gisse2r_rcp85_era2050_2036to2065.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_gisse2r_rcp85_era2075_2061to2090.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_gisse2r_rcp85_era2095_2086to2100.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_ipslcm5alr_rcp45_era2025_2011to2040.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_ipslcm5alr_rcp45_era2050_2036to2065.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_ipslcm5alr_rcp45_era2075_2061to2090.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_ipslcm5alr_rcp45_era2095_2086to2100.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_ipslcm5alr_rcp85_era2025_2011to2040.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_ipslcm5alr_rcp85_era2050_2036to2065.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_ipslcm5alr_rcp85_era2075_2061to2090.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_ipslcm5alr_rcp85_era2095_2086to2100.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_mricgcm3_rcp45_era2025_2011to2040.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_mricgcm3_rcp45_era2050_2036to2065.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_mricgcm3_rcp45_era2075_2061to2090.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_mricgcm3_rcp45_era2095_2086to2100.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_mricgcm3_rcp85_era2025_2011to2040.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_mricgcm3_rcp85_era2050_2036to2065.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_mricgcm3_rcp85_era2075_2061to2090.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_mricgcm3_rcp85_era2095_2086to2100.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_ncarccsm4_rcp45_era2025_2011to2040.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_ncarccsm4_rcp45_era2050_2036to2065.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_ncarccsm4_rcp45_era2075_2061to2090.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_ncarccsm4_rcp45_era2095_2086to2100.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_ncarccsm4_rcp85_era2025_2011to2040.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_ncarccsm4_rcp85_era2050_2036to2065.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_ncarccsm4_rcp85_era2075_2061to2090.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_ncarccsm4_rcp85_era2095_2086to2100.tif\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "ls input_data/Permafrost/geotiff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8014acdd",
   "metadata": {},
   "source": [
    "These outputs look good - but we still have to process the historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "718812f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('input_data/Permafrost/geotiff/magt_cruts31_historical_era1995_1986to2005.tif'), PosixPath('input_data/Permafrost/geotiff/alt_cruts31_historical_era1995_1986to2005.tif')]\n"
     ]
    }
   ],
   "source": [
    "historical_suffix = \"cruts31_historical_era1995_1986to2005.tif\"\n",
    "target_dir = Path(\"input_data/Permafrost/Base/\")\n",
    "out_dir = Path(\"input_data/Permafrost/geotiff/\")\n",
    "hist_asc_fps = [target_dir.joinpath(fp.name) for fp in target_dir.glob(\"*.asc\")]\n",
    "out_names = [out_dir.joinpath(\"magt_\" + historical_suffix),\n",
    "             out_dir.joinpath(\"alt_\" + historical_suffix)]\n",
    "print(out_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2871896b",
   "metadata": {},
   "source": [
    "The convention is `variable_model_scenario_era_yearRange` although it is a little weird here because for the two historical files we are calling CRU TS 3.1 a \"model\" and \"historical\" a scenario. That is just the way it is when jamming together historical baselines and projected futures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a48a2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for asc, gtiff in zip(hist_asc_fps, out_names):\n",
    "    arr, profile = read_raster(asc)\n",
    "    profile.update(driver=\"GTiff\", compress=\"lzw\")\n",
    "    write_raster(arr, gtiff, profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4f623cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[38;5;13malt_cruts31_historical_era1995_1986to2005.tif\u001b[0m\r\n",
      "\u001b[38;5;13mmagt_cruts31_historical_era1995_1986to2005.tif\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "ls input_data/Permafrost/geotiff | grep historical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733af2bb",
   "metadata": {},
   "source": [
    "OK we have succesfully assembled 82 GeoTIFFs from the ASCII raster data. The last move here is to force these to a known Alaskan spatial extent. The data do not cover Canada anyway, so this doesn't reduce the value of the dataset, but will just help it mesh with our other data holdings. We could this on the first creation of the GeoTIFFs but I'll just the use the GDAL one liner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b596a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alaska_Coast_Simplified_Polygon.dbf  Alaska_Coast_Simplified_Polygon.shp.xml\r\n",
      "Alaska_Coast_Simplified_Polygon.prj  Alaska_Coast_Simplified_Polygon.shx\r\n",
      "Alaska_Coast_Simplified_Polygon.shp\r\n"
     ]
    }
   ],
   "source": [
    "ls input_data/Permafrost/clipper_shp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54e54b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa26634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the file paths to the data\n",
    "data_dir = Path(\"geotiff3338/\")\n",
    "data_fps = sorted(data_dir.glob(\"*\"))\n",
    "data_fps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e49ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables needed to describe the data\n",
    "varnames = [\"magt\", \"alt\"]\n",
    "scenarios = [\"historical\", \"rcp45\", \"rcp85\"]\n",
    "models = [\"cruts31\", \"gfdlcm3\", \"gisse2r\", \"ipslcm5alr\", \"mricgcm3\", \"ncarccsm4\"]\n",
    "eras = [\"1995\", \"2025\", \"2050\", \"2075\", \"2095\"]\n",
    "era_starts = [\"1986\", \"2011\", \"2036\", \"2061\", \"2086\"]\n",
    "era_ends = [\"2005\", \"2040\", \"2065\", \"2090\", \"2100\"]\n",
    "units_lu = {\"magt\": \"°C\", \"alt\": \"m\"}\n",
    "\n",
    "# integer encoding for strings for the netcdf coords (Rasdaman wants this)\n",
    "# restart from 0? if it fails, try beginning with zero for each encoding dictionary.\n",
    "era_encoding = {\"1995\": 0, \"2025\": 1, \"2050\": 2, \"2075\": 3, \"2095\": 4}\n",
    "model_encoding = {\"gisse2r\": 0, \"cruts31\": 1, \"gfdlcm3\": 2, \"ipslcm5alr\": 3, \"mricgcm3\": 4, \"ncarccsm4\": 5}\n",
    "scenario_encoding = {\"rcp85\": 0, \"rcp45\": 1, \"historical\": 2}\n",
    "all_encoding = {**units_lu, **era_encoding, **scenario_encoding, **model_encoding}\n",
    "\n",
    "# get x and y dimensions from a single file\n",
    "with rio.open(data_fps[0]) as src:\n",
    "    src_meta = src.meta.copy()\n",
    "    # get x and y coordinates for axes\n",
    "    y = np.array([src.xy(i, 0)[1] for i in np.arange(src.height)])\n",
    "    x = np.array([src.xy(0, j)[0] for j in np.arange(src.width)])\n",
    "    # get the number of pixels\n",
    "    ny, nx = src.height, src.width\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b372724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary from all the raster files\n",
    "# the directory of rasters to dict is kind of boilerplate at this point\n",
    "# key is the filename, value is a subdictionary with keys for each characteristic\n",
    "# we'll force -9999.0 as the no data value for good measure.\n",
    "data_di = {}\n",
    "\n",
    "for fp in data_fps:\n",
    "    fn = fp.name.split(\".tif\")[0]\n",
    "    data_di[fn] = {}\n",
    "    fn_components = fn.split(\"_\")\n",
    "    data_di[fn][\"varname\"] = fn_components[0]\n",
    "    data_di[fn][\"model\"] = fn_components[1]\n",
    "    data_di[fn][\"scenario\"] = fn_components[2]\n",
    "    data_di[fn][\"era\"] = fn_components[3][-4:]\n",
    "    data_di[fn][\"era start\"] = fn_components[4][0:4]\n",
    "    data_di[fn][\"era end\"] = fn_components[4][-4:]\n",
    "    \n",
    "    with rio.open(fp) as src:\n",
    "    \n",
    "        arr = src.read(1)\n",
    "        arr[np.isnan(arr)] = -9999.0\n",
    "        data_di[fn][\"arr\"] = arr\n",
    "\n",
    "data_di['alt_gfdlcm3_rcp45_era2025_2011to2040']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e61b95d",
   "metadata": {},
   "source": [
    "### Higher Dimensions\n",
    "This is where it gets interesting. We need to define the shape of our data cube on a per variable basis. In this instance that'll be era X model X scenario X x-coordinate X y-coordinate. That's a 5 dimensional *hypercube* for those scoring at home (space (x and y) plus time (era) plus model and scenario). There may be a simpler way to do this, but setting up arrays full of no data (e.g. -9999) is a good start and will act as governor when it comes to pushing data because if we exceed the indicies of the array, numpy will yell at us. It is also a memory check - but that shouldn't be an issue on Apollo / Zeus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8222663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a multidimensional array\n",
    "arr_shape = (len(eras),\n",
    "             len(models),\n",
    "             len(scenarios),\n",
    "             ny,\n",
    "             nx)\n",
    "\n",
    "out_arr = np.full(arr_shape, -9999.0, dtype=np.float32)\n",
    "print(out_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6ecd54",
   "metadata": {},
   "source": [
    "This place-holder array checks out. 5 possible era, 6 possible models, 3 possible scenarios. Specifying `dtype` here is important. This should match the `dtype` of the input GeoTIFFs. We are not done initializing arrays though. The hypercube needs to get filled, even when data does not exist because of invalid dimensional combinations. For example, we have no \"historical-ncarccsm4\" scenario-model combinatiion GeoTIFF (because it is nonsense). But should create an array we can push to the hypercube for those indicies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b4a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a \"null\" array for invalid dimensional combos by grabbing a slice of the place-holder array\n",
    "null_arr = out_arr[0, 0, 0,].copy()\n",
    "print(null_arr.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa632a0",
   "metadata": {},
   "source": [
    "Now we convert the dictionary full of raster data to a DataFrame where each row is a file and columns reflect the data and the describing characteristics. I'm not convinced this step is totally necessary, but querying a dictionary, especially a nested dictionary, is sort of fraught. The DataFrame is a bit more friendly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36724f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data_di).sort_index().T\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be498aee",
   "metadata": {},
   "source": [
    "This DataFrame checks out. Next a nested loop will populate a copy of the place-holder `out_arr` for each data variable (MAGT and ALT in this case). The key thing here is the ORDER. We have to be certain that we are iterating in sync with the shape of the place-holder array. We defined our output data structure so we have to stick to it. Era is the first (technically 0th) dimension, model is the second, and scenario the third."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defd5152",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_arrs_by_var = []\n",
    "\n",
    "for var in varnames:\n",
    "    arr_to_fill = out_arr.copy()\n",
    "    for era, er in zip(eras, range(out_arr.shape[0])):\n",
    "        for model, mn in zip(models, range(out_arr.shape[1])):\n",
    "            for scenario, sc in zip(scenarios, range(out_arr.shape[2])):\n",
    "                query = \"era == @era & scenario == @scenario & model == @model\"\n",
    "                try:\n",
    "                    sub_arr = df[df.varname == var].query(query)[\"arr\"].values[0]\n",
    "                except IndexError:\n",
    "                    sub_arr = null_arr.copy()\n",
    "                arr_to_fill[er, mn, sc] = sub_arr\n",
    "                \n",
    "    out_arrs_by_var.append(arr_to_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02f9199",
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae691de",
   "metadata": {},
   "outputs": [],
   "source": [
    "magt_arr = np.array(out_arrs_by_var[0])\n",
    "alt_arr = np.array(out_arrs_by_var[1])\n",
    "print(magt_arr.shape, alt_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d48cdb4",
   "metadata": {},
   "source": [
    "Looks good! A 5 dimensional array for each variable: 5 possible era, 6 possible models, 3 possible scenarios, X, Y. Now we'll create an xarray Dataset object and prescribe the dimensions. We'll use the integer encoding for the coordinate values to play nice with Rasdaman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640ca8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_names = [\"era\", \"model\", \"scenario\", \"y\", \"x\"]\n",
    "\n",
    "ds = xr.Dataset(data_vars={\"magt\": (dim_names, magt_arr),\n",
    "                           \"alt\": (dim_names, alt_arr)},\n",
    "                coords={\"era\": [era_encoding[era] for era in eras],\n",
    "                        \"model\": [model_encoding[model] for model in models],\n",
    "                        \"scenario\": [scenario_encoding[scenario] for scenario in scenarios],\n",
    "                        \"y\": y,\n",
    "                        \"x\": x},\n",
    "               attrs=all_encoding)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831a8670",
   "metadata": {},
   "source": [
    "This is a quick test of the historical ALT data as read straight from the original GeoTIFF and as sliced from the cube to be sure they are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7de84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_hist_slice = ds.sel(era=0, model=7, scenario=13).alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b39530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_hist_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188cbf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(alt_hist_slice.data))\n",
    "print(alt_hist_slice.dtype)\n",
    "print(alt_hist_slice.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38831b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_fps[0])\n",
    "src = rio.open(data_fps[0])\n",
    "test_arr = src.read(1)\n",
    "test_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b2e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "(alt_hist_slice.data == test_arr).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ed972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify encoding to compress\n",
    "encoding = {\"magt\": {\"zlib\": True, \"complevel\": 9, \"_FillValue\": -9999.0},\n",
    "            \"alt\": {\"zlib\": True, \"complevel\": 9, \"_FillValue\": -9999.0},\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a4cd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_netcdf(\"gipl_alt_magt_4km.nc\", encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6909efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -lhrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6933a9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
