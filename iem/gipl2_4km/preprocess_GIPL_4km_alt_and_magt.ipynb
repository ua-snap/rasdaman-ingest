{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbae3a09",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "The purpose of this notebook is to retreive and pre-process the GIPL 2.0 model outputs described by the following publication:\n",
    "\n",
    "<i>\n",
    "Climate damages to Alaska public infrastructure\n",
    "April M. Melvin, Peter Larsen, Brent Boehlert, James E. Neumann, Paul Chinowsky, Xavier Espinet, Jeremy Martinich, Matthew S. Baumann, Lisa Rennels, Alexandra Bothner, Dmitry J. Nicolsky, Sergey S. Marchenko\n",
    "Proceedings of the National Academy of Sciences Jan 2017, 114 (2) E122-E131; DOI: 10.1073/pnas.1611056113\n",
    "</i>\n",
    "\n",
    "The goal is ingest the data as a netCDF to Rasdaman and then create an endpoint around the data in the SNAP Data API for use in the [Northern Climate Reports](https://northerclimatereports.org) app.\n",
    "\n",
    "\n",
    "## Background and Motivation\n",
    "Data represent RCP 4.5 and 8.5 scenarios across 5 climate models. Outputs are binned into \"eras\" and there are two GIPL permafrost variables Active Layer Thickness (ALT) and Mean Annual Ground Temperature (MAGT).\n",
    "The goal is to create a completely inclusive datacube of both historical and projected data. \n",
    "It will have the following dimensions for both ALT and MAGT variables:\n",
    "* era\n",
    "* model\n",
    "* scenario\n",
    "* Y\n",
    "* X\n",
    "\n",
    "# Steps\n",
    " - Download data from the [Google Drive folder shared by Jeremy Littell](https://drive.google.com/file/d/1NaZInm20tpnv63NcC5_p3xTWp5gC3Ty6/view?usp=sharing]) to the `input_data` directory.\n",
    " - Convert the ASCII raster data to GeoTIFF using the accompanying projection information contained in the conjugate .prj file. There is one .prj file for each ASCII file, but they are all the same.\n",
    " - \"Buffer\" the data via a grey dilation morphological operation to extend the data throughout coastal locations.\n",
    " - Encode era, climate model, and emission scenario information in the GeoTIFF file name.\n",
    " - ~~Crop the GeoTiffs to a [known Alaskan spatial boundary](https://github.com/ua-snap/geospatial-vector-veracity/tree/main/vector_data/polygon/boundaries/alaska_coast_simplified)~~ We are omitting this step because the crop, combined with coarse grid size (4 km), ends up considerably reducing the availability of data for coastal locations.\n",
    " - Convert the GeoTIFF stack to a multidimensional netCDF for ingest to Rasdaman.\n",
    " \n",
    "\n",
    "## References\n",
    " - [Melvin et al., 2017Z](https://doi.org/10.1073/pnas.1611056113)\n",
    "   - [Supporting Information](https://www.pnas.org/content/pnas/suppl/2016/12/21/1611056113.DCSupplemental/pnas.201611056SI.pdf?targetid=nameddest%3DSTXT)\n",
    " - [README: ASCII file naming convention key (J. Littel)](https://docs.google.com/document/d/1OdoOqu8pFLnjbyWC_0f5ev8fS-ilZ3pD/edit#heading=h.gjdgxs) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a002c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls input_data/Permafrost/Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f5e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from scipy.ndimage import grey_dilation\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be69c8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the CRS of the ASCII data and the general shape of each file\n",
    "ascii_proj = %cat input_data/Permafrost/Projections/MAGT_10_1.prj\n",
    "header = !head -n 6 input_data/Permafrost/Projections/MAGT_10_1.asc\n",
    "src = rio.open('input_data/Permafrost/Projections/MAGT_10_1.asc')\n",
    "meta = src.meta\n",
    "profile = src.profile\n",
    "shape = src.read(1).shape\n",
    "ascii_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd88b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a9062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shape)\n",
    "print(profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e816d4",
   "metadata": {},
   "source": [
    "Wow! Rasterio knows where to look when the file names of the .prj and .asc files have indentical prefixes. These data appear to be in uniform shape and rasterio \"knows\" how to handle them. No red flags here. Per the README doc referenced earlier, we'll need a look-up table to retain era / model / scenario info and to generate output filenames.\n",
    "\n",
    "```\n",
    "These asci files are labeled by Variable_<Model/Policy Combination>_<Era>\n",
    "\n",
    "Model-Scenario Combinations (10-19)\n",
    "10: MRI-CGCM3, rcp4.5\n",
    "11: MRI-CGCM3, rcp8.5\n",
    "12: IPSL-CM5A-LR, rcp4.5\n",
    "13: IPSL-CM5A-LR, rcp8.5\n",
    "14: GISS-E2-R, rcp 4.5\n",
    "15: GISS-E2-R, rcp 8.5\n",
    "16: GFDL-CM3, rcp4.5\n",
    "17: GFDL-CM3, rcp8.5\n",
    "18: CCSM4, rcp4.5\n",
    "19: CCSM4, rcp8.5\n",
    "Eras (1-4): 30 year eras surrounding the following center dates, except last era is truncated at 2100.\n",
    "1: 2025 (2011 - 2040)\n",
    "2: 2050 (2036 - 2065)\n",
    "3: 2075 (2061 – 2090)\n",
    "4: 2095 (2086 – 2100)\n",
    "\n",
    "The historical baseline is 1986-2005.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdaa5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scenario_di = {10: \"MRICGCM3_rcp45\",\n",
    "                     11: \"MRICGCM3_rcp85\",\n",
    "                     12: \"IPSLCM5ALR_rcp45\",\n",
    "                     13: \"IPSLCM5ALR_rcp85\",\n",
    "                     14: \"GISSE2R_rcp45\",\n",
    "                     15: \"GISSE2R_rcp85\",\n",
    "                     16: \"GFDLCM3_rcp45\",\n",
    "                     17: \"GFDLCM3_rcp85\",\n",
    "                     18: \"NCARCCSM4_rcp45\",\n",
    "                     19: \"NCARCCSM4_rcp85\",\n",
    "                    }\n",
    "era_di = {1: \"era2025_2011to2040\",\n",
    "          2: \"era2050_2036to2065\",\n",
    "          3: \"era2075_2061to2090\",\n",
    "          4: \"era2095_2086to2100\",\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf7c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab all the file paths for PROJECTED data (historical data (n=2) are in a separate directory)\n",
    "target_dir = Path(\"input_data/Permafrost/Projections/\")\n",
    "out_dir = Path(\"input_data/Permafrost/geotiff/\")\n",
    "asc_fps = [fp.name for fp in target_dir.glob(\"*_*.asc\")]\n",
    "cmip5_asc_fps = [target_dir.joinpath(x) for x in asc_fps if int(x.split(\"_\")[1]) >= 10]\n",
    "n_cmip5_ascs = len(cmip5_asc_fps)\n",
    "print(n_cmip5_ascs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c152ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_new_filenames(output_dst, fp_list):\n",
    "    \"\"\"\n",
    "    Blapping together new filenames of the format variable_model_scenario_era.tif\n",
    "    \"\"\"\n",
    "    new_fps = []\n",
    "    for fp in fp_list:\n",
    "        pf_var, model_scenario_key, era_key = fp.name[:-4].split(\"_\")\n",
    "        new_fp = pf_var.lower() + \"_\" + model_scenario_di[int(model_scenario_key)].lower() + \"_\" + era_di[int(era_key)] + \".tif\"\n",
    "        new_fp = output_dst.joinpath(new_fp)\n",
    "        new_fps.append(new_fp)\n",
    "    return new_fps\n",
    "\n",
    "\n",
    "def read_raster(raster_fp):\n",
    "    \"\"\"\n",
    "    Read raster to numpy array.\n",
    "    Read a single raster with rasterio and store raster in memory as a numpy\n",
    "    array. Also reads and returns the profile which includes metadata and the neccessary information\n",
    "    to write congruent datasets.\n",
    "    Args:\n",
    "        raster_fp (str): filepath to raster\n",
    "    Returns:\n",
    "        arr (ndarray): array of raster values\n",
    "        profile (dict): metadata profile\n",
    "    \"\"\"\n",
    "\n",
    "    src = rio.open(raster_fp)\n",
    "    arr = src.read(1)\n",
    "    profile = src.profile\n",
    "    return (arr, profile)\n",
    "\n",
    "\n",
    "def force_nan_to_neg9999(arr):\n",
    "    arr[np.isnan(arr)] = -9999.0\n",
    "    return arr\n",
    "\n",
    "\n",
    "def extend_data_to_coast(arr):\n",
    "    dilated_arr = grey_dilation(arr, 3)\n",
    "    mask = (arr == -9999.0)\n",
    "    new_arr = np.copy(arr)\n",
    "    new_arr[mask] = dilated_arr[mask]\n",
    "    return new_arr\n",
    "\n",
    "\n",
    "def write_raster(arr, outpath, profile):\n",
    "    \"\"\"\n",
    "    Write numpy array to disk as a raster with correct metadata.\n",
    "    Args:\n",
    "        arr (ndarray): array of raster values\n",
    "        outpath (str): output filename and path for raster\n",
    "        profile (dict): metadata for output\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "\n",
    "    with rio.open(outpath, 'w', **profile) as dst:\n",
    "        dst.write(arr, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6356df83",
   "metadata": {},
   "outputs": [],
   "source": [
    "geotiff_fps = make_new_filenames(out_dir, cmip5_asc_fps)\n",
    "print(geotiff_fps[0])\n",
    "print(geotiff_fps[44])\n",
    "print(geotiff_fps[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc6647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the ASCI data to geotiff\n",
    "#!mkdir input_data/Permafrost/geotiff\n",
    "for asc, gtiff in zip(cmip5_asc_fps, geotiff_fps):\n",
    "    arr, profile = read_raster(asc)\n",
    "    arr = force_nan_to_neg9999(arr)\n",
    "    arr = extend_data_to_coast(arr)\n",
    "    profile.update(driver=\"GTiff\", compress=\"lzw\")\n",
    "    write_raster(arr, gtiff, profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d24cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls input_data/Permafrost/geotiff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8014acdd",
   "metadata": {},
   "source": [
    "These outputs look good - but we still have to process the historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718812f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_suffix = \"cruts31_historical_era1995_1986to2005.tif\"\n",
    "target_dir = Path(\"input_data/Permafrost/Base/\")\n",
    "out_dir = Path(\"input_data/Permafrost/geotiff/\")\n",
    "hist_asc_fps = [target_dir.joinpath(fp.name) for fp in target_dir.glob(\"*.asc\")]\n",
    "out_names = [out_dir.joinpath(\"alt_\" + historical_suffix),\n",
    "             out_dir.joinpath(\"magt_\" + historical_suffix)]\n",
    "print(hist_asc_fps)\n",
    "print(out_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2871896b",
   "metadata": {},
   "source": [
    "The convention is `variable_model_scenario_era_yearRange` although it is a little weird here because for the two historical files we are calling CRU TS 3.1 a \"model\" and \"historical\" a scenario. That is just the way it is when jamming together historical baselines and projected futures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a48a2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for asc, gtiff in zip(hist_asc_fps, out_names):\n",
    "    arr, profile = read_raster(asc)\n",
    "    arr = force_nan_to_neg9999(arr)\n",
    "    arr = extend_data_to_coast(arr)\n",
    "    profile.update(driver=\"GTiff\", compress=\"lzw\")\n",
    "    write_raster(arr, gtiff, profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f623cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls input_data/Permafrost/geotiff | grep historical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733af2bb",
   "metadata": {},
   "source": [
    "OK we have succesfully assembled 82 GeoTIFFs from the ASCII raster data. The last move here is to force these to a known Alaskan spatial extent. The data do not cover Canada anyway, so this doesn't reduce the value of the dataset, but will just help it mesh with our other data holdings. We could this on the first creation of the GeoTIFFs but I'll just the use the GDAL one liner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b596a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skipping right now\n",
    "#ls input_data/Permafrost/clipper_shp/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0ee26d",
   "metadata": {},
   "source": [
    "~~Execute the crop script to use the above shapefile to bound the raster extent:\n",
    "`bash crop_geotiff_dir_to_akpoly.sh`~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa26634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the GeoTIFFs are prepared, we can move on to creating a single netCDF\n",
    "# if you to create a cropped dataset, just change the below path\n",
    "data_dir = Path(\"input_data/Permafrost/geotiff/\")\n",
    "data_fps = sorted(data_dir.glob(\"*\"))\n",
    "data_fps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e49ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables needed to describe the data\n",
    "varnames = [\"magt\", \"alt\"]\n",
    "scenarios = [\"historical\", \"rcp45\", \"rcp85\"]\n",
    "models = [\"cruts31\", \"gfdlcm3\", \"gisse2r\", \"ipslcm5alr\", \"mricgcm3\", \"ncarccsm4\"]\n",
    "eras = [\"1995\", \"2025\", \"2050\", \"2075\", \"2095\"]\n",
    "era_starts = [\"1986\", \"2011\", \"2036\", \"2061\", \"2086\"]\n",
    "era_ends = [\"2005\", \"2040\", \"2065\", \"2090\", \"2100\"]\n",
    "units_lu = {\"magt\": \"°C\", \"alt\": \"m\"}\n",
    "\n",
    "# integer encoding for strings for the netcdf coords (Rasdaman wants this)\n",
    "era_encoding = {\"1995\": 0, \"2025\": 1, \"2050\": 2, \"2075\": 3, \"2095\": 4}\n",
    "model_encoding = {\"cruts31\": 0, \"gfdlcm3\": 1, \"gisse2r\": 2, \"ipslcm5alr\": 3, \"mricgcm3\": 4, \"ncarccsm4\": 5}\n",
    "scenario_encoding = {\"historical\": 0, \"rcp45\": 1, \"rcp85\": 2}\n",
    "all_encoding = {**units_lu, **era_encoding, **scenario_encoding, **model_encoding}\n",
    "\n",
    "# get x and y dimensions from a single file\n",
    "with rio.open(data_fps[0]) as src:\n",
    "    src_meta = src.meta.copy()\n",
    "    # get x and y coordinates for axes\n",
    "    y = np.array([src.xy(i, 0)[1] for i in np.arange(src.height)])\n",
    "    x = np.array([src.xy(0, j)[0] for j in np.arange(src.width)])\n",
    "    # get the number of pixels\n",
    "    ny, nx = src.height, src.width "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b372724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary from all the raster files\n",
    "# the directory of rasters to dict is kind of boilerplate at this point\n",
    "# key is the filename, value is a subdictionary with keys for each characteristic\n",
    "# we'll force -9999.0 as the no data value for good measure.\n",
    "data_di = {}\n",
    "\n",
    "for fp in data_fps:\n",
    "    fn = fp.name.split(\".tif\")[0]\n",
    "    data_di[fn] = {}\n",
    "    fn_components = fn.split(\"_\")\n",
    "    data_di[fn][\"varname\"] = fn_components[0]\n",
    "    data_di[fn][\"model\"] = fn_components[1]\n",
    "    data_di[fn][\"scenario\"] = fn_components[2]\n",
    "    data_di[fn][\"era\"] = fn_components[3][-4:]\n",
    "    data_di[fn][\"era start\"] = fn_components[4][0:4]\n",
    "    data_di[fn][\"era end\"] = fn_components[4][-4:]\n",
    "    \n",
    "    with rio.open(fp) as src:\n",
    "    \n",
    "        arr = src.read(1)\n",
    "        arr[np.isnan(arr)] = -9999.0\n",
    "        data_di[fn][\"arr\"] = arr\n",
    "\n",
    "data_di['alt_gfdlcm3_rcp45_era2025_2011to2040']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e61b95d",
   "metadata": {},
   "source": [
    "### Higher Dimensions\n",
    "This is where it gets interesting. We need to define the shape of our data cube on a per variable basis. In this instance that'll be era X model X scenario X x-coordinate X y-coordinate. That's a 5 dimensional *hypercube* for those scoring at home (space (x and y) plus time (era) plus model and scenario). There may be a simpler way to do this, but setting up arrays full of no data (e.g. -9999) is a good start and will act as governor when it comes to pushing data because if we exceed the indicies of the array, numpy will yell at us. It is also a memory check - but that shouldn't be an issue on Apollo / Zeus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8222663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a multidimensional array\n",
    "arr_shape = (len(eras),\n",
    "             len(models),\n",
    "             len(scenarios),\n",
    "             ny,\n",
    "             nx)\n",
    "\n",
    "out_arr = np.full(arr_shape, -9999.0, dtype=np.float32)\n",
    "print(out_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6ecd54",
   "metadata": {},
   "source": [
    "This place-holder array checks out. 5 possible era, 6 possible models, 3 possible scenarios. Specifying `dtype` here is important. This should match the `dtype` of the input GeoTIFFs. We are not done initializing arrays though. The hypercube needs to get filled, even when data does not exist because of invalid dimensional combinations. For example, we have no \"historical-ncarccsm4\" scenario-model combinatiion GeoTIFF (because it is nonsense). But should create an array we can push to the hypercube for those indicies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b4a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a \"null\" array for invalid dimensional combos by grabbing a slice of the place-holder array\n",
    "null_arr = out_arr[0, 0, 0,].copy()\n",
    "print(null_arr.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa632a0",
   "metadata": {},
   "source": [
    "Now we convert the dictionary full of raster data to a DataFrame where each row is a file and columns reflect the data and the describing characteristics. I'm not convinced this step is totally necessary, but querying a dictionary, especially a nested dictionary, is sort of fraught. The DataFrame is a bit more friendly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36724f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data_di).sort_index().T\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be498aee",
   "metadata": {},
   "source": [
    "This DataFrame checks out. Next a nested loop will populate a copy of the place-holder `out_arr` for each data variable (MAGT and ALT in this case). The key thing here is the ORDER. We have to be certain that we are iterating in sync with the shape of the place-holder array. We defined our output data structure so we have to stick to it. Era is the first (technically 0th) dimension, model is the second, and scenario the third."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defd5152",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_arrs_by_var = []\n",
    "\n",
    "for var in varnames:\n",
    "    arr_to_fill = out_arr.copy()\n",
    "    for era, er in zip(eras, range(out_arr.shape[0])):\n",
    "        for model, mn in zip(models, range(out_arr.shape[1])):\n",
    "            for scenario, sc in zip(scenarios, range(out_arr.shape[2])):\n",
    "                query = \"era == @era & scenario == @scenario & model == @model\"\n",
    "                try:\n",
    "                    sub_arr = df[df.varname == var].query(query)[\"arr\"].values[0]\n",
    "                except IndexError:\n",
    "                    sub_arr = null_arr.copy()\n",
    "                arr_to_fill[er, mn, sc] = sub_arr\n",
    "                \n",
    "    out_arrs_by_var.append(arr_to_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02f9199",
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae691de",
   "metadata": {},
   "outputs": [],
   "source": [
    "magt_arr = np.array(out_arrs_by_var[0])\n",
    "alt_arr = np.array(out_arrs_by_var[1])\n",
    "print(magt_arr.shape, alt_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d48cdb4",
   "metadata": {},
   "source": [
    "Looks good! A 5 dimensional array for each variable: 5 possible era, 6 possible models, 3 possible scenarios, Y, X. Now we'll create an xarray Dataset object and prescribe the dimensions. We'll use the integer encoding for the coordinate values to play nice with Rasdaman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640ca8df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dim_names = [\"era\", \"model\", \"scenario\", \"y\", \"x\"]\n",
    "\n",
    "ds = xr.Dataset(data_vars={\"magt\": (dim_names, magt_arr),\n",
    "                           \"alt\": (dim_names, alt_arr)},\n",
    "                coords={\"era\": [era_encoding[era] for era in eras],\n",
    "                        \"model\": [model_encoding[model] for model in models],\n",
    "                        \"scenario\": [scenario_encoding[scenario] for scenario in scenarios],\n",
    "                        \"y\": y,\n",
    "                        \"x\": x},\n",
    "               attrs=all_encoding)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831a8670",
   "metadata": {},
   "source": [
    "This is a quick test of the historical ALT data as read straight from the original GeoTIFF and as sliced from the cube to be sure they are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7de84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_slice = ds.sel(era=1, model=2, scenario=1).alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b39530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188cbf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(test_slice.data))\n",
    "print(test_slice.dtype)\n",
    "print(test_slice.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38831b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = rio.open(\"input_data/Permafrost/geotiff/alt_gisse2r_rcp45_era2025_2011to2040.tif\")\n",
    "test_arr = src.read(1)\n",
    "print(src.meta)\n",
    "print(type(test_arr))\n",
    "print(test_arr.dtype)\n",
    "print(test_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b2e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_slice.data == test_arr).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ed972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify encoding to compress\n",
    "encoding = {\"magt\": {\"zlib\": True, \"complevel\": 9, \"_FillValue\": -9999.0},\n",
    "            \"alt\": {\"zlib\": True, \"complevel\": 9, \"_FillValue\": -9999.0},\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a4cd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_netcdf(\"gipl_alt_magt_4km.nc\", encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6909efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -lhrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fc718c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
